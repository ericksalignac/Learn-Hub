{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama PDF RAG Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.00043797492981\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "inicial_time = time.time()\n",
    "\n",
    "time.sleep(5)\n",
    "final_time = time.time()\n",
    "\n",
    "time_spent = final_time - inicial_time\n",
    "\n",
    "print(time_spent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:57:24.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.338 No runtime found, using MemoryCacheStorageManager\n",
      "2024-12-05 22:57:24.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "2024-12-05 22:57:24.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 22:57:24.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "UnhashableParamError",
     "evalue": "Cannot hash argument 'models_info' (of type `ollama._types.ListResponse`) in 'extract_model_names'.\n\nTo address this, you can tell Streamlit not to hash this argument by adding a\nleading underscore to the argument's name in the function signature:\n\n```\n@st.cache_resource\ndef extract_model_names(_models_info, ...):\n    ...\n```\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:563\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     reduce_data \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reduce__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\copyreg.py:76\u001b[0m, in \u001b[0;36m_reduce_ex\u001b[1;34m(self, proto)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot pickle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m state \u001b[38;5;241m=\u001b[39m base(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'function' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUnhashableTypeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:430\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# we call update_hash twice here, first time for `arg_name`\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# without `hash_funcs`, and second time for `arg_value` with hash_funcs\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# to evaluate user defined `hash_funcs` only for computing `arg_value` hash.\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:160\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, cache_type, hash_source, hash_funcs)\u001b[0m\n\u001b[0;32m    159\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CacheFuncHasher(cache_type, hash_funcs)\n\u001b[1;32m--> 160\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:568\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:565\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n",
      "\u001b[1;31mUnhashableTypeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnhashableParamError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 356\u001b[0m\n\u001b[0;32m    352\u001b[0m                 st\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload a PDF file or use the sample PDF to begin chat...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 356\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 211\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Get available models\u001b[39;00m\n\u001b[0;32m    210\u001b[0m models_info \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mlist()\n\u001b[1;32m--> 211\u001b[0m available_models \u001b[38;5;241m=\u001b[39m \u001b[43mextract_model_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Create layout\u001b[39;00m\n\u001b[0;32m    214\u001b[0m col1, col2 \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mcolumns([\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:217\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m spinner(message, _cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:231\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[1;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mget_function_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_key)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Generate the key for the cached value. This is based on the\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# arguments passed to the function.\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m value_key \u001b[38;5;241m=\u001b[39m \u001b[43m_make_value_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39msuppress(CacheKeyNotFoundError):\n\u001b[0;32m    240\u001b[0m     cached_result \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mread_result(value_key)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:438\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    430\u001b[0m         update_hash(\n\u001b[0;32m    431\u001b[0m             arg_value,\n\u001b[0;32m    432\u001b[0m             hasher\u001b[38;5;241m=\u001b[39margs_hasher,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    435\u001b[0m             hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    436\u001b[0m         )\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnhashableParamError(cache_type, func, arg_name, arg_value, exc)\n\u001b[0;32m    440\u001b[0m value_key \u001b[38;5;241m=\u001b[39m args_hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[0;32m    441\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache key: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, value_key)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py:430\u001b[0m, in \u001b[0;36m_make_value_key\u001b[1;34m(cache_type, func, func_args, func_kwargs, hash_funcs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     update_hash(\n\u001b[0;32m    422\u001b[0m         arg_name,\n\u001b[0;32m    423\u001b[0m         hasher\u001b[38;5;241m=\u001b[39margs_hasher,\n\u001b[0;32m    424\u001b[0m         cache_type\u001b[38;5;241m=\u001b[39mcache_type,\n\u001b[0;32m    425\u001b[0m         hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# we call update_hash twice here, first time for `arg_name`\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# without `hash_funcs`, and second time for `arg_value` with hash_funcs\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# to evaluate user defined `hash_funcs` only for computing `arg_value` hash.\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnhashableTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableParamError(cache_type, func, arg_name, arg_value, exc)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:160\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, cache_type, hash_source, hash_funcs)\u001b[0m\n\u001b[0;32m    157\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mhash_source \u001b[38;5;241m=\u001b[39m hash_source\n\u001b[0;32m    159\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CacheFuncHasher(cache_type, hash_funcs)\n\u001b[1;32m--> 160\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    321\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:568\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mdigest()\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:343\u001b[0m, in \u001b[0;36m_CacheFuncHasher.update\u001b[1;34m(self, hasher, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:325\u001b[0m, in \u001b[0;36m_CacheFuncHasher.to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    321\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\erick.salignac\\Desktop\\Estudo\\Local rag\\local_rag\\Lib\\site-packages\\streamlit\\runtime\\caching\\hashing.py:565\u001b[0m, in \u001b[0;36m_CacheFuncHasher._to_bytes\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    563\u001b[0m     reduce_data \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__reduce__()\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnhashableTypeError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m reduce_data:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(h, item)\n",
      "\u001b[1;31mUnhashableParamError\u001b[0m: Cannot hash argument 'models_info' (of type `ollama._types.ListResponse`) in 'extract_model_names'.\n\nTo address this, you can tell Streamlit not to hash this argument by adding a\nleading underscore to the argument's name in the function signature:\n\n```\n@st.cache_resource\ndef extract_model_names(_models_info, ...):\n    ...\n```\n            "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Streamlit application for PDF-based Retrieval-Augmented Generation (RAG) using Ollama + LangChain.\n",
    "\n",
    "This application allows users to upload a PDF, process it,\n",
    "and then ask questions about the content using a selected language model.\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Set protobuf environment variable to avoid error messages\n",
    "# This might cause some issues with latency but it's a tradeoff\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# Streamlit page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Ollama PDF RAG Streamlit UI\",\n",
    "    page_icon=\"üéà\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"collapsed\",\n",
    ")\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=True)\n",
    "def extract_model_names(\n",
    "    models_info: Dict[str, List[Dict[str, Any]]],\n",
    ") -> Tuple[str, ...]:\n",
    "    \"\"\"\n",
    "    Extract model names from the provided models information.\n",
    "\n",
    "    Args:\n",
    "        models_info (Dict[str, List[Dict[str, Any]]]): Dictionary containing information about available models.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, ...]: A tuple of model names.\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting model names from models_info\")\n",
    "    models_data = [{\"name\": model.model} for model in models_info.models]\n",
    "    model_names = tuple(model[\"name\"] for model in models_data)\n",
    "    logger.info(f\"Extracted model names: {model_names}\")\n",
    "    return model_names\n",
    "\n",
    "\n",
    "def create_vector_db(file_upload) -> Chroma:\n",
    "    \"\"\"\n",
    "    Create a vector database from an uploaded PDF file.\n",
    "\n",
    "    Args:\n",
    "        file_upload (st.UploadedFile): Streamlit file upload object containing the PDF.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: A vector store containing the processed document chunks.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating vector DB from file upload: {file_upload.name}\")\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "    path = os.path.join(temp_dir, file_upload.name)\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(file_upload.getvalue())\n",
    "        logger.info(f\"File saved to temporary path: {path}\")\n",
    "        loader = UnstructuredPDFLoader(path)\n",
    "        data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    logger.info(\"Document split into chunks\")\n",
    "\n",
    "    # Updated embeddings configuration\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"myRAG\"\n",
    "    )\n",
    "    logger.info(\"Vector DB created\")\n",
    "\n",
    "    shutil.rmtree(temp_dir)\n",
    "    logger.info(f\"Temporary directory {temp_dir} removed\")\n",
    "    return vector_db\n",
    "\n",
    "\n",
    "def process_question(question: str, vector_db: Chroma, selected_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a user question using the vector database and selected language model.\n",
    "\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "        vector_db (Chroma): The vector database containing document embeddings.\n",
    "        selected_model (str): The name of the selected language model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response to the user's question.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing question: {question} using model: {selected_model}\")\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOllama(model=selected_model)\n",
    "    \n",
    "    # Query prompt template\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "        different versions of the given user question to retrieve relevant documents from\n",
    "        a vector database. By generating multiple perspectives on the user question, your\n",
    "        goal is to help the user overcome some of the limitations of the distance-based\n",
    "        similarity search. Provide these alternative questions separated by newlines.\n",
    "        Original question: {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    # Set up retriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(), \n",
    "        llm,\n",
    "        prompt=QUERY_PROMPT\n",
    "    )\n",
    "\n",
    "    # RAG prompt template\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Create chain\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = chain.invoke(question)\n",
    "    logger.info(\"Question processed and response generated\")\n",
    "    return response\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def extract_all_pages_as_images(file_upload) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Extract all pages from a PDF file as images.\n",
    "\n",
    "    Args:\n",
    "        file_upload (st.UploadedFile): Streamlit file upload object containing the PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of image objects representing each page of the PDF.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting all pages as images from file: {file_upload.name}\")\n",
    "    pdf_pages = []\n",
    "    with pdfplumber.open(file_upload) as pdf:\n",
    "        pdf_pages = [page.to_image().original for page in pdf.pages]\n",
    "    logger.info(\"PDF pages extracted as images\")\n",
    "    return pdf_pages\n",
    "\n",
    "\n",
    "def delete_vector_db(vector_db: Optional[Chroma]) -> None:\n",
    "    \"\"\"\n",
    "    Delete the vector database and clear related session state.\n",
    "\n",
    "    Args:\n",
    "        vector_db (Optional[Chroma]): The vector database to be deleted.\n",
    "    \"\"\"\n",
    "    logger.info(\"Deleting vector DB\")\n",
    "    if vector_db is not None:\n",
    "        vector_db.delete_collection()\n",
    "        st.session_state.pop(\"pdf_pages\", None)\n",
    "        st.session_state.pop(\"file_upload\", None)\n",
    "        st.session_state.pop(\"vector_db\", None)\n",
    "        st.success(\"Collection and temporary files deleted successfully.\")\n",
    "        logger.info(\"Vector DB and related session state cleared\")\n",
    "        st.rerun()\n",
    "    else:\n",
    "        st.error(\"No vector database found to delete.\")\n",
    "        logger.warning(\"Attempted to delete vector DB, but none was found\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the Streamlit application.\n",
    "    \"\"\"\n",
    "    st.subheader(\"üß† Ollama PDF RAG playground\", divider=\"gray\", anchor=False)\n",
    "\n",
    "    # Get available models\n",
    "    models_info = ollama.list()\n",
    "    available_models = extract_model_names(models_info)\n",
    "\n",
    "    # Create layout\n",
    "    col1, col2 = st.columns([1.5, 2])\n",
    "\n",
    "    # Initialize session state\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state[\"messages\"] = []\n",
    "    if \"vector_db\" not in st.session_state:\n",
    "        st.session_state[\"vector_db\"] = None\n",
    "    if \"use_sample\" not in st.session_state:\n",
    "        st.session_state[\"use_sample\"] = False\n",
    "\n",
    "    # Model selection\n",
    "    if available_models:\n",
    "        selected_model = col2.selectbox(\n",
    "            \"Pick a model available locally on your system ‚Üì\", \n",
    "            available_models,\n",
    "            key=\"model_select\"\n",
    "        )\n",
    "\n",
    "    # Add checkbox for sample PDF\n",
    "    use_sample = col1.toggle(\n",
    "        \"Use sample PDF (Scammer Agent Paper)\", \n",
    "        key=\"sample_checkbox\"\n",
    "    )\n",
    "    \n",
    "    # Clear vector DB if switching between sample and upload\n",
    "    if use_sample != st.session_state.get(\"use_sample\"):\n",
    "        if st.session_state[\"vector_db\"] is not None:\n",
    "            st.session_state[\"vector_db\"].delete_collection()\n",
    "            st.session_state[\"vector_db\"] = None\n",
    "            st.session_state[\"pdf_pages\"] = None\n",
    "        st.session_state[\"use_sample\"] = use_sample\n",
    "\n",
    "    if use_sample:\n",
    "        # Use the sample PDF\n",
    "        sample_path = \"scammer-agent.pdf\"\n",
    "        if os.path.exists(sample_path):\n",
    "            if st.session_state[\"vector_db\"] is None:\n",
    "                with st.spinner(\"Processing sample PDF...\"):\n",
    "                    loader = UnstructuredPDFLoader(file_path=sample_path)\n",
    "                    data = loader.load()\n",
    "                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "                    chunks = text_splitter.split_documents(data)\n",
    "                    st.session_state[\"vector_db\"] = Chroma.from_documents(\n",
    "                        documents=chunks,\n",
    "                        embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "                        collection_name=\"myRAG\"\n",
    "                    )\n",
    "                    # Open and display the sample PDF\n",
    "                    with pdfplumber.open(sample_path) as pdf:\n",
    "                        pdf_pages = [page.to_image().original for page in pdf.pages]\n",
    "                        st.session_state[\"pdf_pages\"] = pdf_pages\n",
    "        else:\n",
    "            st.error(\"Sample PDF file not found in the current directory.\")\n",
    "    else:\n",
    "        # Regular file upload with unique key\n",
    "        file_upload = col1.file_uploader(\n",
    "            \"Selecione um arquivo PDF ‚Üì\", \n",
    "            type=\"pdf\", \n",
    "            accept_multiple_files=False,\n",
    "            key=\"pdf_uploader\"\n",
    "        )\n",
    "\n",
    "        if file_upload:\n",
    "            if st.session_state[\"vector_db\"] is None:\n",
    "                with st.spinner(\"Processando upload do PDF...\"):\n",
    "                    st.session_state[\"vector_db\"] = create_vector_db(file_upload)\n",
    "                    pdf_pages = extract_all_pages_as_images(file_upload)\n",
    "                    st.session_state[\"pdf_pages\"] = pdf_pages\n",
    "\n",
    "    # Display PDF if pages are available\n",
    "    if \"pdf_pages\" in st.session_state and st.session_state[\"pdf_pages\"]:\n",
    "        # PDF display controls\n",
    "        zoom_level = col1.slider(\n",
    "            \"Zoom Level\", \n",
    "            min_value=100, \n",
    "            max_value=1000, \n",
    "            value=700, \n",
    "            step=50,\n",
    "            key=\"zoom_slider\"\n",
    "        )\n",
    "\n",
    "        # Display PDF pages\n",
    "        with col1:\n",
    "            with st.container(height=410, border=True):\n",
    "                # Removed the key parameter from st.image()\n",
    "                for page_image in st.session_state[\"pdf_pages\"]:\n",
    "                    st.image(page_image, width=zoom_level)\n",
    "\n",
    "    # Delete collection button\n",
    "    delete_collection = col1.button(\n",
    "        \"‚ö†Ô∏è Deletar collection\", \n",
    "        type=\"secondary\",\n",
    "        key=\"delete_button\"\n",
    "    )\n",
    "\n",
    "    if delete_collection:\n",
    "        delete_vector_db(st.session_state[\"vector_db\"])\n",
    "\n",
    "    # Chat interface\n",
    "    with col2:\n",
    "        message_container = st.container(height=500, border=True)\n",
    "\n",
    "        # Display chat history\n",
    "        for i, message in enumerate(st.session_state[\"messages\"]):\n",
    "            avatar = \"ü§ñ\" if message[\"role\"] == \"assistant\" else \"üòé\"\n",
    "            with message_container.chat_message(message[\"role\"], avatar=avatar):\n",
    "                st.markdown(message[\"content\"])\n",
    "\n",
    "        # Chat input and processing\n",
    "        if prompt := st.chat_input(\"Enter a prompt here...\", key=\"chat_input\"):\n",
    "            try:\n",
    "                # Add user message to chat\n",
    "                st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "                with message_container.chat_message(\"user\", avatar=\"üòé\"):\n",
    "                    st.markdown(prompt)\n",
    "\n",
    "                # Process and display assistant response\n",
    "                with message_container.chat_message(\"assistant\", avatar=\"ü§ñ\"):\n",
    "                    with st.spinner(\":green[processing...]\"):\n",
    "                        if st.session_state[\"vector_db\"] is not None:\n",
    "                            response = process_question(\n",
    "                                prompt, st.session_state[\"vector_db\"], selected_model\n",
    "                            )\n",
    "                            st.markdown(response)\n",
    "                        else:\n",
    "                            st.warning(\"Please upload a PDF file first.\")\n",
    "\n",
    "                # Add assistant response to chat history\n",
    "                if st.session_state[\"vector_db\"] is not None:\n",
    "                    st.session_state[\"messages\"].append(\n",
    "                        {\"role\": \"assistant\", \"content\": response}\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(e, icon=\"‚õîÔ∏è\")\n",
    "                logger.error(f\"Error processing prompt: {e}\")\n",
    "        else:\n",
    "            if st.session_state[\"vector_db\"] is None:\n",
    "                st.warning(\"Fa√ßa upload de um arquivo PDF ou selecione o exemplo para come√ßar o chat...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='mxbai-embed-large:latest', modified_at=datetime.datetime(2024, 12, 4, 23, 32, 36, 632141, tzinfo=TzInfo(-03:00)), digest='468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8', size=669615493, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='334M', quantization_level='F16')), Model(model='llava:7b', modified_at=datetime.datetime(2024, 12, 4, 22, 18, 59, 109732, tzinfo=TzInfo(-03:00)), digest='8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081', size=4733363377, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='7B', quantization_level='Q4_0')), Model(model='llava:13b', modified_at=datetime.datetime(2024, 12, 4, 21, 44, 42, 194135, tzinfo=TzInfo(-03:00)), digest='0d0eb4d7f485d7d0a21fd9b0c1d5b04da481d2150a097e81b64acb59758fdef6', size=8011256494, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='13B', quantization_level='Q4_0')), Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2024, 11, 29, 10, 14, 6, 62303, tzinfo=TzInfo(-03:00)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 11, 28, 15, 52, 41, 773080, tzinfo=TzInfo(-03:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 23:05:49 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "2024-12-05 23:05:49.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 23:05:49.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 23:05:49.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 23:05:49 - INFO - Extracting model names from models_info\n",
      "2024-12-05 23:05:49 - INFO - Extracted model names: ('mxbai-embed-large:latest', 'llava:7b', 'llava:13b', 'nomic-embed-text:latest', 'llama3.2:latest')\n",
      "2024-12-05 23:05:49.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-05 23:05:49.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: attention.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "local_path = \"attention.pdf\"\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "    print(f\"PDF loaded successfully: {local_path}\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 51 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and retrieval\n",
    "local_model = \"llama3.2\"  # or whichever model you prefer\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template = \"\"\"Voc√™ √© um assistente de modelo de linguagem AI. Sua tarefa √© gerar 2\n",
    "    vers√µes diferentes da pergunta do usu√°rio para recuperar documentos relevantes de\n",
    "    um banco de dados vetorial. Ao gerar m√∫ltiplas perspectivas sobre a pergunta do usu√°rio,\n",
    "    seu objetivo √© ajudar o usu√°rio a superar algumas das limita√ß√µes da busca por similaridade baseada em dist√¢ncia. \n",
    "    Forne√ßa essas perguntas alternativas separadas por novas linhas.\n",
    "    Pergunta original: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Responda a pergunta baseado APENAS no contexto a seguir::\n",
    "{context}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "De acordo com o seu documento, a aten√ß√£o (ou attention) √© uma fun√ß√£o matem√°tica utilizada em modelos de processamento de linguagem e aprendizado de m√°quina. Ela permite que um modelo \"olhe\" para diferentes partes de um texto ou sequ√™ncia e d√™ mais peso √†quela que √© mais relevante para o tarefa espec√≠fica. Isso √© feito atrav√©s de uma opera√ß√£o matem√°tica chamada aten√ß√£o escalada ao produto ponto (Scaled Dot-Product Attention)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"O que √© aten√ß√£o segundo meu documento?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A ideia principal deste documento parece ser relacionada √† implementa√ß√£o e avalia√ß√£o de modelos de processamento do linguagem baseados na aten√ß√£o, especificamente o Transformer, que foi proposto por Vaswani et al. (2017) e desenvolvido pela equipe liderada por Ashish, Jakob, Noam, Llion, Lukasz e Aidan. O documento apresenta resultados de experimenta√ß√£o com v√°rios modelos variados, incluindo a avalia√ß√£o da efic√°cia do Transformer em tarefas de processamento do linguagem, como o tradu√ß√£o e o questionamento sem√¢ntico.\n",
       "\n",
       "Al√©m disso, o documento tamb√©m aborda quest√µes relacionadas √† compreens√£o da aten√ß√£o em diferentes n√≠veis de representa√ß√£o da linguagem, como a aten√ß√£o no n√≠vel de palavras (word-level), frases (sentence-level) e documentos (document-level).\n",
       "\n",
       "No entanto, sem mais informa√ß√µes espec√≠ficas sobre a estrutura do documento e o contexto em que foi usado, n√£o √© poss√≠vel fornecer uma resposta mais detalhada."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1\n",
    "chat_with_pdf(\"Qual √© a ideia principal desse documento?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "O conceito de aten√ß√£o em redes neurais foi desenvolvido pela primeira vez por Sepp Hochreiter e J√ºrgen Schmidhuber em 2001, quando apresentaram o papel do gradient flow na aprendizagem de redes neuronais recorrentes. No entanto, o conceito moderno de aten√ß√£o foi introduzido pelo pesquisador japon√™s Mikoto Okunawa em 1997, e mais tarde desenvolvido pela equipe liderada por Yoshua Bengio.\n",
       "\n",
       "A aten√ß√£o √© um m√©todo utilizado para reorganizar a forma como as redes neuronais processam informa√ß√µes. Ela permite √†s redes focar melhor nas partes do input que s√£o mais relevantes para o processo de aprendizado, em vez de processar todas as informa√ß√µes de uma s√≥ vez.\n",
       "\n",
       "No contexto da aprendizagem de m√°quinas, a aten√ß√£o √© aplicada para melhorar a performance dos modelos de seq√º√™ncia, como l√≠ngua natural processamento e tradu√ß√£o. Ela funciona ao permitir √†s redes neuronais focar melhor nas partes do input que s√£o mais relevantes para o processo de aprendizado.\n",
       "\n",
       "A t√©cnica foi inicialmente utilizada em redes neuronais recorrentes (RNNs) e mais tarde se generalizou para outras tipos de redes neuronais, incluindo redes neuronais convolucionais (CNNs).\n",
       "\n",
       "O conceito de aten√ß√£o √© aplicado de v√°rias maneiras para melhorar a performance dos modelos:\n",
       "\n",
       "*   **Aten√ß√£o de peso**: Em vez de processar todas as informa√ß√µes do input de uma s√≥ vez, a aten√ß√£o de peso permite √†s redes focar melhor nas partes mais relevantes.\n",
       "*   **Aten√ß√£o spatial**: Al√©m da aten√ß√£o de peso, a aten√ß√£o spatial permite √†s redes focar melhor em regi√µes espec√≠ficas do input.\n",
       "*   **Aten√ß√£o temporal**: A aten√ß√£o temporal permite √†s redes focar melhor em momentos espec√≠ficos do input.\n",
       "\n",
       "A aten√ß√£o √© uma t√©cnica poderosa que tem sido amplamente utilizada em muitas aplica√ß√µes da aprendizagem de m√°quina, incluindo processamento de l√≠ngua natural, tradu√ß√£o e vis√£o por computador."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2\n",
    "chat_with_pdf(\"De onde surgiu o conceito de aten√ß√£o? E como √© aplicado para melhorar a performace dos modelos?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up when done \n",
    "vector_db.delete_collection()\n",
    "print(\"Vector database deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
